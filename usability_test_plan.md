# Paper Analysis System: Usability Test Plan

## Introduction

This document outlines the usability testing approach for the Paper Analysis System, a web application designed to help users search, analyze, and visualize academic papers from arXiv. The usability testing will evaluate the system's effectiveness, efficiency, and user satisfaction.

## Test Objectives

1. Evaluate the overall user experience of the Paper Analysis System
2. Identify usability issues and areas for improvement
3. Assess the intuitiveness of the arXiv search interface
4. Evaluate the clarity and usefulness of visualization components
5. Determine if users can successfully complete common tasks without assistance
6. Gather feedback on system performance and response times

## Participant Profile

We will recruit 8-12 participants with the following characteristics:
- Academic researchers (graduate students, professors, research assistants)
- Age range: 22-65
- Mix of genders and research disciplines
- Varying levels of experience with academic paper search systems:
  - Novice users (minimal experience with academic search systems)
  - Intermediate users (occasional users of academic search systems)
  - Expert users (frequent users of academic search systems)

## Testing Environment

- Testing will be conducted both in-person and remotely
- In-person tests will take place in a controlled environment with screen and audio recording
- Remote tests will use video conferencing with screen sharing
- Participants will use the system through a standard web browser
- Both desktop and mobile interfaces will be tested

## Tasks

Participants will be asked to complete the following tasks:

### Task 1: Basic arXiv Paper Search
- Navigate to the arXiv search page
- Search for papers by author "Feynman"
- Identify and open the most relevant result
- View the full paper details

### Task 2: Advanced Search and Filtering
- Perform a search for papers about "quantum computing"
- Filter results by relevance
- Try different search types (keyword, author, title)
- Identify papers that have positive sentiment

### Task 3: Paper Details and PDF Access
- Search for a specific paper
- View detailed information about the paper
- Access the PDF link
- Return to the search results

### Task 4: Paper Clustering
- Search for papers on "machine learning"
- Enable clustering with 3 clusters
- Interpret the clustering visualization
- Identify the main topics in each cluster

### Task 5: arXiv Metadata Analysis
- Navigate to the arXiv analysis dashboard
- Analyze the distribution of papers by category
- Identify the most active authors
- Examine publication trends over time
- Adjust the sample size and observe changes

## Metrics

### Effectiveness
- Task completion rate (percentage of tasks completed successfully)
- Error rate (number of errors made during task completion)
- Number of requests for assistance

### Efficiency
- Time to complete each task
- Number of clicks/actions to complete each task
- Navigation path analysis

### Satisfaction
- Post-task satisfaction ratings (5-point scale)
- System Usability Scale (SUS) questionnaire
- Net Promoter Score (NPS)
- Open-ended feedback on likes, dislikes, and suggestions

## Test Procedure

1. **Introduction (5 minutes)**
   - Welcome participant
   - Explain purpose of the test
   - Obtain informed consent
   - Clarify that the system is being tested, not the participant

2. **Pre-test Interview (5 minutes)**
   - Gather demographic information
   - Assess prior experience with academic search systems
   - Understand research needs and habits

3. **Task Completion (30-45 minutes)**
   - Present tasks one at a time
   - Ask participant to think aloud while completing tasks
   - Observe and record interactions
   - Note areas of confusion or frustration
   - Collect post-task feedback

4. **Post-test Questionnaire (10 minutes)**
   - System Usability Scale (SUS) questionnaire
   - Net Promoter Score question
   - Rating of specific features

5. **Post-test Interview (10 minutes)**
   - Open-ended questions about the experience
   - Discussion of most useful and problematic features
   - Suggestions for improvement

## Data Collection and Analysis

Data will be collected through:
- Screen and audio recordings
- Observer notes
- Task completion metrics
- Questionnaire responses
- Interview transcripts

Analysis will include:
- Quantitative analysis of task completion rates, times, and error rates
- Statistical analysis of questionnaire responses
- Qualitative analysis of observations and feedback
- Prioritization of identified usability issues by severity and frequency

## Test Materials

1. **Moderator Script**
   - Structured guide for test facilitator
   - Consistent instructions for all participants

2. **Participant Consent Form**
   - Explanation of the test purpose
   - Description of data collection methods
   - Privacy and confidentiality assurances

3. **Task Scenarios**
   - Detailed descriptions of tasks
   - Success criteria for each task

4. **Pre-test Questionnaire**
   - Demographics
   - Experience with academic search systems
   - Research habits

5. **Post-task Questions**
   - Ease of completion rating
   - Satisfaction with the process
   - Specific issues encountered

6. **Post-test Questionnaire**
   - System Usability Scale (SUS)
   - Feature satisfaction ratings
   - Net Promoter Score

7. **Observer Forms**
   - Task completion tracking
   - Error tracking
   - Notable quotes and observations

## Focus Areas for Two Main Pages

### 1. arXiv Search Page (arxiv.html)
- Search form usability and clarity
- Search results presentation
- Paper details modal functionality
- Sentiment visualization effectiveness
- Clustering visualization interpretation
- Pagination navigation
- Mobile responsiveness

### 2. arXiv Analysis Dashboard (arxiv_analysis.html)
- Data visualization clarity
- Interactive chart controls
- Analysis parameter adjustments
- Metadata statistics understanding
- Information hierarchy and organization
- Loading performance
- Mobile responsiveness

## Schedule

- **Pilot Testing**: 1 week before main testing
  - Test with 2 participants
  - Refine tasks and procedures

- **Main Testing Sessions**: 2-week period
  - 2-3 sessions per day
  - 60 minutes per session

- **Analysis and Reporting**: 1 week
  - Compile and analyze results
  - Generate usability report
  - Develop recommendations

## Reporting

The final usability test report will include:
1. Executive summary
2. Test methodology
3. Participant demographics
4. Task performance results
5. Key findings and usability issues
6. Satisfaction metrics
7. Recommendations for improvement
8. Appendices with raw data

## Responsibilities

- **Test Coordinator**: Schedule sessions, recruit participants, manage logistics
- **Test Facilitator**: Conduct usability test sessions, guide participants
- **Observer**: Take notes, monitor recordings, track task completion
- **Data Analyst**: Compile and analyze test results, prepare report

## Next Steps

Following the usability testing:
1. Development team reviews findings
2. Prioritize issues for resolution
3. Implement improvements
4. Consider follow-up testing after significant changes